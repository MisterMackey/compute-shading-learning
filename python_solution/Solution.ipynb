{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a59b4bea-800a-47f9-8821-298ee3404637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/09 21:49:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/09 21:49:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().appName(\"Analysis\").master(\"local[12]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21eccc57-6f88-4b62-b1ea-b82794a47265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Id='533b409c-48ab-4222-ae1e-9da42cfbf78b', Notional=760954, Interest Rate=1.8259983840056635, Reset Frequency=9, Start Date=datetime.datetime(2018, 2, 28, 10, 32, 4, 557479), Term=25, Remaining Notional=611299.7133333334, Payment Type='Linear', Risk Indicator=0, Next Reset Date=datetime.datetime(2027, 2, 26, 10, 32, 4, 557479))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"../test_data.parquet\")\n",
    "df.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2466cf66-0500-4db3-adb1-aee649d50ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, lit\n",
    "from datetime import datetime, timedelta\n",
    "ASSUMED_DATE_TODAY=datetime(year=2023,month=1,day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7482a02b-74d9-405b-9b78-2fbab0594429",
   "metadata": {},
   "outputs": [],
   "source": [
    "annuities = df.filter(df['Payment_Type'] == \"Annuity\")\n",
    "linears = df.filter(df['Payment_Type'] == \"Linear\")\n",
    "bullets = df.filter(df['Payment_Type'] == \"Bullet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb633e27-0542-424e-b897-e7d886194564",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearpayments = linears.rdd.map(\n",
    "    lambda x: (x['Id'], x['Notional'] / x['Term']))\n",
    "\n",
    "linearpayments = linearpayments.toDF([\"f_Id\", \"monthly_repayments\"]) \n",
    "#linearpayments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd5a4d4-c533-4d4c-a871-1b37e0246415",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulletpayments = bullets.rdd.map(\n",
    "    lambda x: (x[\"Id\"], 0.0)\n",
    ").toDF([\"f_Id\", \"monthly_payments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e421c9-b243-4932-889f-954f2c949c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annuity_payment(notional, interest, term):\n",
    "    monthsTotal = term * 12\n",
    "    r = interest / 12 / 100\n",
    "    payAmount = (notional*r*(pow(1+r, monthsTotal))) / (pow(1+r, monthsTotal) -1)\n",
    "    return payAmount\n",
    "\n",
    "annuitypayments = annuities.rdd.map(\n",
    "    lambda x: (x[\"Id\"], calc_annuity_payment(x[\"Notional\"], x[\"Interest_Rate\"], x[\"Term\"]) )\n",
    ").toDF([\"f_Id\", \"monthly_payments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a346075c-5a0d-4d4e-93b3-d56064aee366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#annuities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52b9a2e-273b-47cf-9fc3-56467e516c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "annuities = annuities.join(annuitypayments, annuities['Id'] == annuitypayments['f_Id'], \"inner\")\n",
    "linears = linears.join(linearpayments, linears['Id'] == linearpayments['f_Id'], \"inner\")\n",
    "bullets = bullets.join(bulletpayments, bullets['Id'] == bulletpayments['f_Id'], \"inner\")\n",
    "df_full = annuities.union(linears).union(bullets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a93b1fd-1c9a-4d57-a798-bae638bae197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.withColumn(\"end_date\", fn.add_months(col(\"Start_Date\"), col(\"Term\")*12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54aac77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create schema for the results table\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DecimalType, IntegerType, DateType\n",
    "schema = StructType([\n",
    "  StructField('Id', StringType(), False),\n",
    "  StructField('Interest_Rate', DecimalType(), False),\n",
    "  StructField('Reset_Frequency', IntegerType(), False),\n",
    "    StructField('Remaining_Notional', DecimalType(), False),\n",
    "    StructField('Risk_Indicator', IntegerType(), False),\n",
    "    StructField('Next_Reset_Date', DateType(), False),\n",
    "    StructField('monthly_payment', DecimalType(), False)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73022b2e-9a56-41db-ae9d-696ab185f327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = false)\n",
      " |-- Interest Rate: decimal(10,0) (nullable = false)\n",
      " |-- Reset Frequency: integer (nullable = false)\n",
      " |-- Remaining Notional: decimal(10,0) (nullable = false)\n",
      " |-- Risk Indicator: integer (nullable = false)\n",
      " |-- Next Reset Date: date (nullable = false)\n",
      " |-- monthly_payment: decimal(10,0) (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment_projection = spark.createDataFrame([], schema)\n",
    "payment_projection.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b87ce-b7a0-4b46-8ce2-de66d1cd7fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f099c5-898d-4c62-b020-9af0d9d81a34",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 11 (1337018053.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    def calc_one_step(original_row: Row, curr_date: datetime, T_Minus_one: Row):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 11\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "riskMigration = {\n",
    "\t0: (0.0, 1 - 0.001),\n",
    "\t1: (0.1, 1 - 0.01),\n",
    "\t2: (0.05, 1 - 0.01),\n",
    "\t3: (0.05, 1 - 0.05),\n",
    "\t4: (0.2, 1 - 0.1),\n",
    "}\n",
    "additionalInterestRatePerDuration = {\n",
    "\t30: 2.2,\n",
    "\t25: 1.9,\n",
    "\t20: 1.5,\n",
    "\t15: 1.0,\n",
    "\t10: 0.5,\n",
    "\t9: 0.4,\n",
    "\t7: 0.1,\n",
    "\t5: 0.0,\n",
    "\n",
    "}\n",
    "additionalInterestRatePerRiskCategory = {\n",
    "\t0: 0.0,\n",
    "\t1: 0.3 ,\n",
    "\t2: 1.1 ,\n",
    "\t3: 1.9 ,\n",
    "\t4: 3.5 ,\n",
    "}\n",
    "\n",
    "def migrate_risk_category(old_category: int):\n",
    "\tdraw = random.random()\n",
    "\tprobabilities = riskMigration[old_category]\n",
    "\tif draw < probabilities[0]:\n",
    "\t\treturn old_category - 1\n",
    "\telif draw > probabilities[1]:\n",
    "\t\treturn old_category + 1\n",
    "\telse:\n",
    "\t\treturn old_category\n",
    "\n",
    "def calc_one_step(original_row: Row, curr_date: datetime, T_Minus_one: Row):\n",
    "\t#fill in t minus one if we are in period 0\n",
    "\tif T_Minus_one is None:\n",
    "\t\tT_Minus_one = original_row\n",
    "\t#migrate the risk\n",
    "\tnewRisk = migrate_risk_category(T_Minus_one['Risk_Indicator'])\n",
    "\t#check if we need to rest the interest rate, using month etc so we dont accidentily miss one if the day differs\n",
    "\tresetFrequency = T_Minus_one['Reset_Frequency']\n",
    "\tinterestRate = T_Minus_one['Interest_Rate']\n",
    "\tresetDate = T_Minus_one['Next_Reset_Date']\n",
    "\tif curr_date.month == T_Minus_one['Next_Reset_Date'].month and curr_date.day == T_Minus_one['Next_Reset_Date'].day:\n",
    "\t\timpliedBaseRate = original_row[\"Interest_Rate\"] - additionalInterestRatePerDuration[original_row['Term']] - additionalInterestRatePerRiskCategory[original_row['Risk_Indicator']]\n",
    "\t\tyearsLeft = original_row['end_date'].year - original_row['Start Date'].year\n",
    "\t\treset_options = [30, 25, 20, 15, 10, 9, 7, 5]\n",
    "\t\tresetFrequency = min([x for x in reset_options if x >= yearsLeft])\n",
    "\t\tresetDate = datetime(year=resetDate.year + yearsLeft, month= resetDate.month, day= resetDate.day)\n",
    "\t\t#new reset frequency is implied to be the new duration\n",
    "\t\tinterestRate = impliedBaseRate + additionalInterestRatePerDuration[resetFrequency] + additionalInterestRatePerRiskCategory[newRisk]\n",
    "\t\tif original_row['Payment_Type'] == 'Annuity':\n",
    "\t\t\t#re-calculate the annuity payments\n",
    "\t\t\t#too tired, continue here later\n",
    "\n",
    "\tif newRisk == 5:\n",
    "\t\tinterest = 0.0\n",
    "\t\trepayment = 0.0\n",
    "\t\twriteOff = T_Minus_one['Remaining_Notional']\n",
    "\telif original_row['Payment_Type'] == 'Bullet':\n",
    "\t\tinterest = (interestRate / 12) * T_Minus_one['Remaining_Notional']\n",
    "\t\trepayment = 0.0\n",
    "\t\twriteOff = 0\n",
    "\telif original_row['Payment_Type'] == 'Linear':\n",
    "\t\tinterest = (interestRate / 12) * T_Minus_one['Remaining_Notional']\n",
    "\t\trepayment = original_row['monthly_payment']\n",
    "\t\twriteOff = 0\n",
    "\telif original_row['Payment_Type'] == 'Annuity':\n",
    "\t\tinterest = (interestRate / 12) * T_Minus_one['Remaining_Notional']\n",
    "\t\trepayment = original_row['monthly_payment'] - interest\n",
    "\t\twriteOff = 0\n",
    "\tremainingNotional = T_Minus_one['Remaining_Notional'] - repayment - writeOff\n",
    "\tnewrow = Row(\n",
    "        \tId=original_row['Id'],\n",
    "        \tInterest_Rate=interestRate,\n",
    "\t\tReset_Frequency=resetFrequency,\n",
    "\t\tRemaining_Notional=remainingNotional,\n",
    "\t\tRisk_Indicator=newRisk,\n",
    "\t\tNext_Reset_Date=resetDate,\n",
    "\t\tmonthly_payment=monthlyPayment\n",
    "\t)\n",
    "\n",
    "\n",
    "def calc_all_periods_for_row(row: Row):\n",
    "    #endDate = row[\"end_date\"][\"end_date\"]\n",
    "    endDate = datetime(2030,1,1)\n",
    "    curr_date = ASSUMED_DATE_TODAY\n",
    "    listresults = []\n",
    "    results = None\n",
    "    while curr_date < endDate:\n",
    "        results = calc_one_step(row, curr_date, results)\n",
    "        listresults.append(results)\n",
    "        curr_date += timedelta(days=31)\n",
    "        curr_date -= (timedelta(days=curr_date.day-1))\n",
    "        #above two lines should get the beginning of the month\n",
    "    totals = spark.createDataFrame(listresults)\n",
    "    payment_projection = payment_projection.union(totals)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddce808b-46b2-4031-bd63-caaf76446e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb4b8de8-0900-43fe-9863-02fac0a39393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(kwargs={'keys': ['foo', 'b a r'], 'values': [1, 2]})\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"keys\": [\"foo\", \"b a r\"], \"values\": [1, 2]}\n",
    "r = Row(kwargs=kwargs)\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
